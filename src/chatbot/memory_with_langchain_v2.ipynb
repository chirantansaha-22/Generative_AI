{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "#from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.messages import HumanMessage,AIMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################################\n",
    "#######  Here we will try to invoke ChatOpenAI for a convesation 2 times and see if \n",
    "#######  the model can recognize the converstation in the past\n",
    "####################################################################################\n",
    "\n",
    "######################################################################################################\n",
    "### OPENAI_API_KEY is defined in the file \".env\" in the project root folder(not checked \n",
    "## in to Git due to privacy concerns)\n",
    "##\n",
    "## You can use your own .env file and define your api keys there and use them in the python file after \n",
    "## loading your environment variables using load_dotenv() and then access the variable\n",
    "######################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello CS! How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\",api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "response = llm.invoke([HumanMessage(\"Hi, My name is CS.\")])\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AIMessage:I'm sorry, I do not have the ability to recall specific information about individual users.\n"
     ]
    }
   ],
   "source": [
    "case1_response2 = llm.invoke(\"Do you recall what is my name?\")\n",
    "print(f\"{case1_response2.__class__.__name__}:{case1_response2.content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################################################\n",
    "### As we can see above, the model does not have the context of the previous conversation\n",
    "### Case 2: Lets add the first AIMessage response in the array and see if it helps\n",
    "############################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, your name is CS.\n"
     ]
    }
   ],
   "source": [
    "case2_response = llm.invoke([\n",
    "    HumanMessage(\"Hi, My name is CS.\"),\n",
    "    AIMessage(\"Hello CS, nice to meet you! How can I assist you today?\"),\n",
    "    HumanMessage(\"Do you recall what is my name?\")\n",
    "])\n",
    "print(case2_response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################\n",
    "## Naive approach: \n",
    "## So, all we need to do is keep a track of the AI responses and pass\n",
    "## it to the model through messages array while invoking the model.\n",
    "##\n",
    "##################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HumanMessage,content='Hi, My name is CS.' additional_kwargs={} response_metadata={}\n",
      "AIMessage,content=\"Hello CS, it's nice to meet you. How can I assist you today?\" additional_kwargs={} response_metadata={}\n",
      "HumanMessage,content='Do you recall what is my name?' additional_kwargs={} response_metadata={}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Yes, your name is CS. How can I help you further?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 15, 'prompt_tokens': 47, 'total_tokens': 62, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-274f62f7-4eb1-4c64-9cd8-4bf45bd36dd3-0', usage_metadata={'input_tokens': 47, 'output_tokens': 15, 'total_tokens': 62, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = []\n",
    "messages.append(HumanMessage(\"Hi, My name is CS.\"))\n",
    "ai_response1 = llm.invoke(messages)\n",
    "\n",
    "# Appending the AI message to teh messages array\n",
    "messages.append(AIMessage(ai_response1.content))\n",
    "\n",
    "# Apending the Human Message\n",
    "messages.append(HumanMessage(\"Do you recall what is my name?\"))\n",
    "\n",
    "for message in messages:\n",
    "    print(f\"{message.__class__.__name__},{message}\")\n",
    "\n",
    "# invoking the model again to check if context was retained\n",
    "llm.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################################################\n",
    "#### So, How can we implement this?\n",
    "####\n",
    "#### As per langchain documentation v0.2, MessageHistory class can be used to wrap our model and make it stateful. \n",
    "#### This will keep track of inputs and outputs of the model, and store them in some datastore. \n",
    "####\n",
    "#### Future interactions will then load those messages and pass them into the chain as part of the input.\n",
    "########################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'chat1': InMemoryChatMessageHistory(messages=[HumanMessage(content='Hello, my name is Chirantan.', additional_kwargs={}, response_metadata={}), AIMessage(content='Hello Chirantan, nice to meet you! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 16, 'total_tokens': 34, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-e319e721-bbb6-478b-9837-5845e4f943fd-0', usage_metadata={'input_tokens': 16, 'output_tokens': 18, 'total_tokens': 34, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})])}\n",
      "Hello Chirantan, nice to meet you! How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory, InMemoryChatMessageHistory\n",
    "\n",
    "store = {}\n",
    "\n",
    "##  This function is expected to take in a session_id and return a Message History object\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = InMemoryChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo\",api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "with_message_history = RunnableWithMessageHistory(model, get_session_history)\n",
    "\n",
    "config = {\"configurable\":{\"session_id\":\"chat1\"}}\n",
    "\n",
    "response = with_message_history.invoke([\n",
    "    HumanMessage(\"Hello, my name is Chirantan.\")],\n",
    "    config = config\n",
    "    )\n",
    "\n",
    "print(store)\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################################################\n",
    "#### Invoking followup invocation by passing the same config and verifying if context is retained.\n",
    "#################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, your name is Chirantan. How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "response2 = with_message_history.invoke([\n",
    "    HumanMessage(\"Hello, do you recall my name?\")],\n",
    "    config = config\n",
    "    )\n",
    "print(response2.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Invoking the LLM by passing a different session_id in config(chat2) and verifying if context is retained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mStore details(containing only reference for chat1:\u001b[0m\n",
      "{'chat1': InMemoryChatMessageHistory(messages=[HumanMessage(content='Hello, my name is Chirantan.', additional_kwargs={}, response_metadata={}), AIMessage(content='Hello Chirantan, nice to meet you! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 16, 'total_tokens': 34, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-e319e721-bbb6-478b-9837-5845e4f943fd-0', usage_metadata={'input_tokens': 16, 'output_tokens': 18, 'total_tokens': 34, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), HumanMessage(content='Hello, do you recall my name?', additional_kwargs={}, response_metadata={}), AIMessage(content='Yes, your name is Chirantan. How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 49, 'total_tokens': 66, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-e23038f0-7c8e-4d65-8e23-de0e44460ae1-0', usage_metadata={'input_tokens': 49, 'output_tokens': 17, 'total_tokens': 66, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]), 'chat2': InMemoryChatMessageHistory(messages=[HumanMessage(content='Do you recall whats my name?', additional_kwargs={}, response_metadata={}), AIMessage(content=\"I'm sorry, I don't have the ability to recall personal information about individuals.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 14, 'total_tokens': 32, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-bbf20982-776a-4faa-8e18-f944afa41d71-0', usage_metadata={'input_tokens': 14, 'output_tokens': 18, 'total_tokens': 32, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), HumanMessage(content='Do you recall whats my name?', additional_kwargs={}, response_metadata={}), AIMessage(content=\"I'm sorry, I'm unable to recall personal information about individuals.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 15, 'prompt_tokens': 46, 'total_tokens': 61, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-8b5461a7-06ff-4a5d-857b-7a3b658c28c1-0', usage_metadata={'input_tokens': 46, 'output_tokens': 15, 'total_tokens': 61, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), HumanMessage(content='Do you recall whats my name?', additional_kwargs={}, response_metadata={}), AIMessage(content=\"I'm sorry, I don't have the ability to remember personal information about users.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 75, 'total_tokens': 93, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-5b4af05a-e89a-4ba4-a291-3153cd3091ba-0', usage_metadata={'input_tokens': 75, 'output_tokens': 18, 'total_tokens': 93, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), HumanMessage(content='Do you recall whats my name?', additional_kwargs={}, response_metadata={}), AIMessage(content=\"I'm sorry, but I don't have the ability to remember personal information about users.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 19, 'prompt_tokens': 107, 'total_tokens': 126, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-498f8b58-b0b5-4006-bde5-ede7ca43e228-0', usage_metadata={'input_tokens': 107, 'output_tokens': 19, 'total_tokens': 126, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), HumanMessage(content='Do you recall whats my name?', additional_kwargs={}, response_metadata={}), AIMessage(content=\"I'm sorry, but I don't have the ability to recall personal information about users.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 19, 'prompt_tokens': 140, 'total_tokens': 159, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-c84cb334-beb2-486d-8fa2-40b08c9551b1-0', usage_metadata={'input_tokens': 140, 'output_tokens': 19, 'total_tokens': 159, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), HumanMessage(content='Do you recall whats my name?', additional_kwargs={}, response_metadata={}), AIMessage(content=\"I'm sorry, I don't have the ability to remember personal information about users.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 173, 'total_tokens': 191, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-4538c174-97d6-4493-ba3c-ec02da1644c0-0', usage_metadata={'input_tokens': 173, 'output_tokens': 18, 'total_tokens': 191, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), HumanMessage(content='Do you recall whats my name?', additional_kwargs={}, response_metadata={}), AIMessage(content=\"I'm sorry, but I don't have the ability to recall personal information about users.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 19, 'prompt_tokens': 205, 'total_tokens': 224, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-f328b92e-0d02-4b45-9965-e846c9893ce9-0', usage_metadata={'input_tokens': 205, 'output_tokens': 19, 'total_tokens': 224, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})])}\n",
      "\n",
      "\u001b[31mResponse when llm is invoked with a different chat(chat2) configuration:\u001b[0m\n",
      "I'm sorry, but I don't have the ability to remember personal information about users.\n"
     ]
    }
   ],
   "source": [
    "from colorama import Fore, Style\n",
    "another_config = {\"configurable\":{\"session_id\":\"chat2\"}}\n",
    "\n",
    "print(f\"{Fore.RED}Store details(containing only reference for chat1:{Style.RESET_ALL}\\n{store}\")\n",
    "\n",
    "print()\n",
    "print(f\"{Fore.RED}Response when llm is invoked with a different chat(chat2) configuration:{Style.RESET_ALL}\")\n",
    "response3 = with_message_history.invoke([HumanMessage(\"Do you recall whats my name?\")],config = another_config)\n",
    "print(response3.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Invoking the LLM with previous config having the previous session_id is able to retrieve the chat history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, I remember you! How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "response4 = with_message_history.invoke(\n",
    "    [HumanMessage(\"Hi there! Do you remmember me?\")],\n",
    "    config = config\n",
    ")\n",
    "\n",
    "print(response4.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making it more personalized and tying this with Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oh Chirantan, what a lovely name,\n",
      "How may I assist you in this moment of fame?\n",
      "Tell me your query, I'll do my best,\n",
      "To help you with any request or test.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.prompts import SystemMessagePromptTemplate\n",
    "\n",
    "store = {}\n",
    "\n",
    "def get_session_history(session_id:str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = InMemoryChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "system_message = SystemMessagePromptTemplate.from_template(\"You are a call centre employee which a knack for poetry.\")\n",
    "\n",
    "# prompt = ChatPromptTemplate.from_messages(\n",
    "#     [(\"system\",\"You are a sales representative which a knack for poetry.\"),\n",
    "#      MessagesPlaceholder(variable_name=\"my_messages_trace\")])\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [system_message,\n",
    "     MessagesPlaceholder(variable_name=\"my_messages_trace\")])\n",
    "\n",
    "lcel_chain = prompt | llm\n",
    "\n",
    "response5 = lcel_chain.invoke({\"my_messages_trace\":[HumanMessage(\"Hello, My name is Chirantan\")]})\n",
    "print(response5.content)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oh, Prem, a name that rings so true,\n",
      "How may I be of service to you?\n",
      "With your request, I'll do my best,\n",
      "To help you through any test.\n"
     ]
    }
   ],
   "source": [
    "with_message_history = RunnableWithMessageHistory(lcel_chain,get_session_history)\n",
    "config = {\"configurable\":{\"session_id\":\"chat6\"}}\n",
    "\n",
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(\"My name is Prem\")],\n",
    "    config = config\n",
    ")\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oh, your name is Prem, like a shining gem,\n",
      "It's lovely to meet you, my poetic friend.\n",
      "How may I assist you on this fine day?\n",
      "Just let me know, and I'll find a way.\n"
     ]
    }
   ],
   "source": [
    "response6 = with_message_history.invoke(\n",
    "    [HumanMessage(\"What is my name?\")],\n",
    "    config = config\n",
    ")\n",
    "\n",
    "print(response6.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
